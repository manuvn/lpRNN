# -*- coding: utf-8 -*-
from __future__ import absolute_import
import numpy as np

from keras import backend as K
from keras import activations
from keras import initializers
from keras import regularizers
from keras import constraints
from keras.engine import InputSpec
from keras.legacy import interfaces
from keras.layers import Recurrent
from keras.utils.generic_utils import get_custom_objects

def _time_distributed_dense(x, w, b=None, dropout=None,
                            input_dim=None, output_dim=None,
                            timesteps=None, training=None):
    """Apply `y . w + b` for every temporal slice y of x.
    # Arguments
        x: input tensor.
        w: weight matrix.
        b: optional bias vector.
        dropout: wether to apply dropout (same dropout mask
            for every temporal slice of the input).
        input_dim: integer; optional dimensionality of the input.
        output_dim: integer; optional dimensionality of the output.
        timesteps: integer; optional number of timesteps.
        training: training phase tensor or boolean.
    # Returns
        Output tensor.
    """
    if not input_dim:
        input_dim = K.shape(x)[2]
    if not timesteps:
        timesteps = K.shape(x)[1]
    if not output_dim:
        output_dim = K.shape(w)[1]

    if dropout is not None and 0. < dropout < 1.:
        # apply the same dropout pattern at every timestep
        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))
        dropout_matrix = K.dropout(ones, dropout)
        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)
        x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)

    # collapse time dimension and batch dimension together
    x = K.reshape(x, (-1, input_dim))
    x = K.dot(x, w)
    if b is not None:
        x = K.bias_add(x, b)
    # reshape to 3D tensor
    if K.backend() == 'tensorflow':
        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))
        x.set_shape([None, None, output_dim])
    else:
        x = K.reshape(x, (-1, timesteps, output_dim))
    return x

def _timegate_init(shape, dtype=None):
        assert len(shape)==2
        return K.constant(np.vstack((np.random.uniform(10, 100, shape[1]),
                      np.random.uniform(0, 1000, shape[1]),
                      np.zeros(shape[1]) + 0.05)),
                      dtype=dtype)

class PlpRNN(Recurrent):
    """lpRNN with timegate (Phased lpRNN).

    # Arguments
        units: Positive integer, dimensionality of the output space.
        retention_ratio: Can specify a specific number to 
            retain state. [0,1).
        learn_retention_ratio: True/False - Enables training 
            update of the retention_ratio values.
         activation: Activation function to use
            (see [activations](../activations.md)).
            If you don't specify anything, no activation is applied
            (ie. "linear" activation: `a(x) = x`).
        recurrent_activation: Activation function to use
            for the recurrent step
            (see [activations](../activations.md)).
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix,
            used for the linear transformation of the inputs.
            (see [initializers](../initializers.md)).
        recurrent_initializer: Initializer for the `recurrent_kernel`
            weights matrix,
            used for the linear transformation of the recurrent state.
            (see [initializers](../initializers.md)).
        bias_initializer: Initializer for the bias vector
            (see [initializers](../initializers.md)).
        unit_forget_bias: Boolean.
            If True, add 1 to the bias of the forget gate at initialization.
            Setting it to true will also force `bias_initializer="zeros"`.
            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        recurrent_regularizer: Regularizer function applied to
            the `recurrent_kernel` weights matrix
            (see [regularizer](../regularizers.md)).
        bias_regularizer: Regularizer function applied to the bias vector
            (see [regularizer](../regularizers.md)).
        activity_regularizer: Regularizer function applied to
            the output of the layer (its "activation").
            (see [regularizer](../regularizers.md)).
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix
            (see [constraints](../constraints.md)).
        recurrent_constraint: Constraint function applied to
            the `recurrent_kernel` weights matrix
            (see [constraints](../constraints.md)).
        bias_constraint: Constraint function applied to the bias vector
            (see [constraints](../constraints.md)).
        dropout: Float between 0 and 1.
            Fraction of the units to drop for
            the linear transformation of the inputs.
        recurrent_dropout: Float between 0 and 1.
            Fraction of the units to drop for
            the linear transformation of the recurrent state.
        alpha: float between 0 and 1. Leak fraction of time gate.
    # References
        - [Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences](https://arxiv.org/abs/1610.09513)
        - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)
        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)
        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)
        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)
    """
    @interfaces.legacy_recurrent_support
    def __init__(self, units,
                 retention_ratio=None, 
                 learn_retention_ratio=False, 
                 activation='tanh',
                 recurrent_activation='hard_sigmoid',
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 recurrent_initializer='orthogonal',
                 bias_initializer='zeros',
                 unit_forget_bias=True,
                 timegate_initializer=_timegate_init,
                 kernel_regularizer=None,
                 recurrent_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 timegate_regularizer=None,
                 kernel_constraint=None,
                 recurrent_constraint=None,
                 bias_constraint=None,
                 timegate_constraint='non_neg',
                 dropout=0.,
                 recurrent_dropout=0.,
                 alpha=0.001,
                 **kwargs):
        super(PlpRNN, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.recurrent_activation = activations.get(recurrent_activation)
        self.use_bias = use_bias

        self.kernel_initializer = initializers.get(kernel_initializer)
        self.recurrent_initializer = initializers.get(recurrent_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.unit_forget_bias = unit_forget_bias
        self.timegate_initializer = initializers.get(timegate_initializer)

        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.activity_regularizer = regularizers.get(activity_regularizer)
        self.timegate_regularizer = regularizers.get(timegate_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.recurrent_constraint = constraints.get(recurrent_constraint)
        self.bias_constraint = constraints.get(bias_constraint)
        self.timegate_constraint = constraints.get(timegate_constraint)

        self.dropout = min(1., max(0., dropout))
        self.recurrent_dropout = min(1., max(0., recurrent_dropout))
        self.alpha = alpha
        self._learn_retention_ratio = learn_retention_ratio
        self._num_units = units
        self.set_retention_ratio = retention_ratio

    def build(self, input_shape):
        if isinstance(input_shape, list):
            input_shape = input_shape[0]

        batch_size = input_shape[0] if self.stateful else None
        self.input_dim = input_shape[2]
        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))
        self.state_spec = [InputSpec(shape=(batch_size, self.units)),
                           InputSpec(shape=(batch_size, self.units))]

        self.states = [None, None, None]
        if self.stateful:
            self.reset_states()

        self.kernel = self.add_weight((self.input_dim, self.units),
                                      name='kernel',
                                      initializer=self.kernel_initializer,
                                      regularizer=self.kernel_regularizer,
                                      constraint=self.kernel_constraint)

        self.recurrent_kernel = self.add_weight(
            (self.units, self.units),
            name='recurrent_kernel',
            initializer=self.recurrent_initializer,
            regularizer=self.recurrent_regularizer,
            constraint=self.recurrent_constraint)

        if self.use_bias:
            self.bias = self.add_weight((self.units,),
                                        name='bias',
                                        initializer=self.bias_initializer,
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
            if self.unit_forget_bias:
                bias_value = np.zeros((self.units,))
                #bias_value[self.units: self.units * 2] = 1.
                K.set_value(self.bias, bias_value)
        else:
            self.bias = None

        #self.kernel_i = self.kernel[:, :self.units]
        #self.kernel_f = self.kernel[:, self.units: self.units * 2]
        self.kernel_c = self.kernel[:, : self.units]
        #self.kernel_o = self.kernel[:, self.units * 3:]

        #self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]
        #self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]
        self.recurrent_kernel_c = self.recurrent_kernel[:,:self.units]
        #self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]

        if self.use_bias:
            #self.bias_i = self.bias[:self.units]
            #self.bias_f = self.bias[self.units: self.units * 2]
            self.bias_c = self.bias[: self.units]
            #self.bias_o = self.bias[self.units * 3:]
        else:
            #self.bias_i = None
            #self.bias_f = None
            self.bias_c = None
            #self.bias_o = None

        # time-gate
        self.timegate_kernel = self.add_weight(
            (3, self.units),
            name='timegate_kernel',
            initializer=self.timegate_initializer,
            regularizer=self.timegate_regularizer,
            constraint=self.timegate_constraint)
    
        if self._learn_retention_ratio == True:
            self._retention_ratio = self.add_weight('ret_ratio', 
                                                      shape=[self._num_units], 
                                                      initializer=keras.initializers.RandomUniform(minval=0.1,maxval=0.99))
        else:    
            if self.set_retention_ratio == None:
                max_ = 1
                min_ = 0.1
                val = (max_ - min_)*np.random.random(self._num_units).astype(np.float32) + min_
                self._retention_ratio = K.constant(val,name='ret_ratio')
            else:
                self._retention_ratio = self.set_retention_ratio
                
        self.built = True

    def preprocess_input(self, inputs, training=None):
        if self.implementation == 0:
            input_shape = K.int_shape(inputs)
            input_dim = input_shape[2]
            timesteps = input_shape[1]

            #x_i = _time_distributed_dense(inputs, self.kernel_i, self.bias_i,
            #                              self.dropout, input_dim, self.units,
            #                              timesteps, training=training)
            #x_f = _time_distributed_dense(inputs, self.kernel_f, self.bias_f,
            #                              self.dropout, input_dim, self.units,
            #                              timesteps, training=training)
            x_c = _time_distributed_dense(inputs, self.kernel_c, self.bias_c,
                                          self.dropout, input_dim, self.units,
                                          timesteps, training=training)
            #x_o = _time_distributed_dense(inputs, self.kernel_o, self.bias_o,
            #                              self.dropout, input_dim, self.units,
            #                              timesteps, training=training)
            #return K.concatenate([x_i, x_f, x_c, x_o], axis=2)
            return x_c
        else:
            return inputs

    def get_constants(self, inputs, training=None):
        constants = []
        if self.implementation == 0 and 0 < self.dropout < 1:
            input_shape = K.int_shape(inputs)
            input_dim = input_shape[-1]
            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))
            ones = K.tile(ones, (1, int(input_dim)))

            def dropped_inputs():
                return K.dropout(ones, self.dropout)

            dp_mask = [K.in_train_phase(dropped_inputs,
                                        ones,
                                        training=training) for _ in range(4)]
            constants.append(dp_mask)
        else:
            constants.append([K.cast_to_floatx(1.) for _ in range(4)])

        if 0 < self.recurrent_dropout < 1:
            ones = K.ones_like(K.reshape(inputs[:, 0, 0], (-1, 1)))
            ones = K.tile(ones, (1, self.units))

            def dropped_inputs():
                return K.dropout(ones, self.recurrent_dropout)
            rec_dp_mask = [K.in_train_phase(dropped_inputs,
                                            ones,
                                            training=training) for _ in range(4)]
            constants.append(rec_dp_mask)
        else:
            constants.append([K.cast_to_floatx(1.) for _ in range(4)])
        return constants

    def step(self, inputs, states):
        h_tm1 = states[0]
        c_tm1 = states[1]
        t_tm1 = states[2]
        dp_mask = states[3]
        rec_dp_mask = states[4]

        # time related variables, simply add +1 to t for now...starting from 0
        # need to find better way if asynchronous/irregular time input is desired
        # such as slicing input where first index is time and using that instead.
        t = t_tm1 + 1
        # using timegate_constraint = 'non_neg' instead
        # self.timegate_kernel = K.abs(self.timegate_kernel)
        period = self.timegate_kernel[0]
        shift = self.timegate_kernel[1]
        r_on = self.timegate_kernel[2]

        # modulo operation not implemented in Tensorflow backend, so write explicitly.
        # a mod n = a - (n * int(a/n))
        # phi = ((t - shift) % period) / period
        phi = ((t - shift) - (period * ((t - shift) // period))) / period

        # K.switch not consistent between Theano and Tensorflow backend, so write explicitly.
        up = K.cast(K.less_equal(phi, r_on * 0.5), K.floatx()) * 2 * phi / r_on
        mid = K.cast(K.less_equal(phi, r_on), K.floatx()) * \
              K.cast(K.greater(phi, r_on * 0.5), K.floatx()) * (2 - (2 * phi / r_on))
        end = K.cast(K.greater(phi, r_on), K.floatx()) * self.alpha * phi
        k = up + mid + end

        if self.implementation == 2:
            z = K.dot(inputs * dp_mask[0], self.kernel)
            z += K.dot(h_tm1 * rec_dp_mask[0], self.recurrent_kernel)
            if self.use_bias:
                z = K.bias_add(z, self.bias)

            #z0 = z[:, :self.units]
            #z1 = z[:, self.units: 2 * self.units]
            z2 = z[:, : self.units]
            #z3 = z[:, 3 * self.units:]

            #i = self.recurrent_activation(z0)
            f = self._retention_ratio #self.recurrent_activation(z1)
            # intermediate cell update
            c_hat = f * c_tm1 + (1-f)* self.activation(z2)
            # final cell update
            c = k * c_hat + (1 - k) * c_tm1
            # o = self.recurrent_activation(z3)
        else:
            if self.implementation == 0:
                # x_i = inputs[:, :self.units]
                # x_f = inputs[:, self.units: 2 * self.units]
                x_c = inputs[:, : self.units]
                #x_o = inputs[:, 3 * self.units:]
            elif self.implementation == 1:
                #x_i = K.dot(inputs * dp_mask[0], self.kernel_i) + self.bias_i
                #x_f = K.dot(inputs * dp_mask[1], self.kernel_f) + self.bias_f
                x_c = K.dot(inputs * dp_mask[2], self.kernel_c) + self.bias_c
                #x_o = K.dot(inputs * dp_mask[3], self.kernel_o) + self.bias_o
            else:
                raise ValueError('Unknown `implementation` mode.')

            #i = self.recurrent_activation(x_i + K.dot(h_tm1 * rec_dp_mask[0],
            #                                          self.recurrent_kernel_i))
            f = self._retention_ratio#self.recurrent_activation(x_f + K.dot(h_tm1 * rec_dp_mask[1],
            #                                          self.recurrent_kernel_f))
            # intermediate cell update
            c_hat = f * c_tm1 + (1-f) * self.activation(x_c + K.dot(h_tm1 * rec_dp_mask[2],
                                                            self.recurrent_kernel_c))
            # final cell update
            c = k * c_hat + (1 - k) * c_tm1
            #o = self.recurrent_activation(x_o + K.dot(h_tm1 * rec_dp_mask[3],
            #                                          self.recurrent_kernel_o))
        # intermediate hidden update
        #h_hat = o * self.activation(c_hat)
        # final hidden update - MVN - Seems pointless to have now
        # h = k * h_hat + (1 - k) * h_tm1
        if 0 < self.dropout + self.recurrent_dropout:
            #h._uses_learning_phase = True
            c._uses_learning_phase = True
        return c, [c, c, t]
        #return h, [h, c, t]

    def get_config(self):
        config = {'units': self.units,
                  'retention_ratio': self._retention_ratio,
                  'learn_retention_ratio': self._learn_retention_ratio,
                  'activation': activations.serialize(self.activation),
                  'recurrent_activation': activations.serialize(self.recurrent_activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer': initializers.serialize(self.kernel_initializer),
                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),
                  'bias_initializer': initializers.serialize(self.bias_initializer),
                  'unit_forget_bias': self.unit_forget_bias,
                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),
                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),
                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),
                  'kernel_constraint': constraints.serialize(self.kernel_constraint),
                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint),
                  'dropout': self.dropout,
                  'recurrent_dropout': self.recurrent_dropout}
        base_config = super(PlpRNN, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


get_custom_objects().update({'PlpRNN': PlpRNN})
